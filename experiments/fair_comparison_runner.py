#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¬å¹³æ¯”è¾ƒå®éªŒè¿è¡Œå™¨
ç¡®ä¿æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„å·¥ä½œæµå®ä¾‹ï¼Œè¿›è¡Œå…¬å¹³çš„æ€§èƒ½æ¯”è¾ƒ
"""

import logging
import pandas as pd
import numpy as np
import torch
from typing import Dict, List, Tuple, Any
from config.config import Config
from config.hyperparameters import Hyperparameters
from models.fe_iddqn import FE_IDDQN
from baselines.traditional_schedulers import FIFOScheduler, SJFScheduler, HEFTScheduler
from baselines.rl_baselines import DQNScheduler, DDQNScheduler, BF_DDQNScheduler
from baselines.meta_heuristics import GAScheduler, PSOScheduler, ACOScheduler
from environment.historical_replay_simulator import HistoricalReplaySimulator
from evaluation.metrics import Evaluator
import json
import os
from datetime import datetime

class FairComparisonRunner:
    """å…¬å¹³æ¯”è¾ƒå®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, data: Dict[str, pd.DataFrame], output_dir: str):
        self.logger = logging.getLogger(__name__)
        self.data = data
        self.output_dir = output_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.evaluator = Evaluator()
        
        # é¢„å®šä¹‰çš„å·¥ä½œæµå®ä¾‹é›†ï¼ˆç¡®ä¿æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒçš„æ•°æ®ï¼‰
        self.fixed_workflow_instances = None
        self._prepare_fixed_workflow_instances()
    
    def _prepare_fixed_workflow_instances(self):
        """å‡†å¤‡å›ºå®šçš„å·¥ä½œæµå®ä¾‹é›†ï¼Œç¡®ä¿æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç›¸åŒæ•°æ®"""
        self.logger.info("å‡†å¤‡å›ºå®šçš„å·¥ä½œæµå®ä¾‹é›†...")
        
        # è·å–æœ‰ä»»åŠ¡çš„è¿›ç¨‹ID
        processes_with_tasks = self.data['task_instance']['process_instance_id'].unique()
        
        # è·å–æˆåŠŸä¸”æœ‰ä»»åŠ¡çš„è¿›ç¨‹å®ä¾‹
        successful_processes = self.data['process_instance'][
            (self.data['process_instance']['state'] == 7) & 
            (self.data['process_instance']['id'].isin(processes_with_tasks))
        ].sort_values('start_time').reset_index(drop=True)
        
        if len(successful_processes) == 0:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°æˆåŠŸçš„å·¥ä½œæµå®ä¾‹ï¼")
            return
        
        # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­é€‰æ‹©å·¥ä½œæµå®ä¾‹
        np.random.seed(Config.RANDOM_SEED)
        max_processes = min(Config.MAX_PROCESSES_PER_EPISODE, len(successful_processes))
        
        # éšæœºé€‰æ‹©ä½†ä¿æŒå›ºå®š
        selected_indices = np.random.choice(len(successful_processes), max_processes, replace=False)
        self.fixed_workflow_instances = successful_processes.iloc[selected_indices].reset_index(drop=True)
        
        self.logger.info(f"å›ºå®šå·¥ä½œæµå®ä¾‹é›†å‡†å¤‡å®Œæˆ: {len(self.fixed_workflow_instances)} ä¸ªå·¥ä½œæµ")
        
        # ä¿å­˜å·¥ä½œæµå®ä¾‹ä¿¡æ¯
        workflow_info = {
            'total_workflows': len(self.fixed_workflow_instances),
            'workflow_ids': self.fixed_workflow_instances['id'].tolist(),
            'workflow_names': self.fixed_workflow_instances['name'].tolist(),
            'total_tasks': len(self.data['task_instance'][
                self.data['task_instance']['process_instance_id'].isin(self.fixed_workflow_instances['id'])
            ]),
            'random_seed': Config.RANDOM_SEED,
            'creation_time': datetime.now().isoformat()
        }
        
        with open(os.path.join(self.output_dir, 'fixed_workflow_instances.json'), 'w', encoding='utf-8') as f:
            json.dump(workflow_info, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"å·¥ä½œæµå®ä¾‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {os.path.join(self.output_dir, 'fixed_workflow_instances.json')}")
    
    def create_simulator_with_fixed_instances(self) -> HistoricalReplaySimulator:
        """åˆ›å»ºä½¿ç”¨å›ºå®šå·¥ä½œæµå®ä¾‹çš„æ¨¡æ‹Ÿå™¨"""
        if self.fixed_workflow_instances is None:
            raise ValueError("å›ºå®šå·¥ä½œæµå®ä¾‹é›†æœªå‡†å¤‡å¥½ï¼")
        
        # åˆ›å»ºæ¨¡æ‹Ÿå™¨ï¼Œä½†è¦†ç›–å…¶å·¥ä½œæµå®ä¾‹
        simulator = HistoricalReplaySimulator(
            process_instances=self.data['process_instance'],
            task_instances=self.data['task_instance'],
            task_definitions=self.data['task_definition'],
            process_task_relations=self.data['process_task_relation']
        )
        
        # å¼ºåˆ¶ä½¿ç”¨å›ºå®šçš„å·¥ä½œæµå®ä¾‹
        simulator.successful_processes = self.fixed_workflow_instances.copy()
        simulator.current_process_idx = 0
        simulator.current_task_idx = 0
        simulator.completed_tasks = set()
        simulator.running_tasks = {}
        simulator.available_resources = {}
        simulator.task_schedule_history = []
        
        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè¿›ç¨‹ï¼ˆä¾èµ–å…³ç³»ä¼šåœ¨_load_current_processä¸­è‡ªåŠ¨å¤„ç†ï¼‰
        simulator._load_current_process()
        
        return simulator
    
    def run_algorithm_comparison(self, algorithms: List[str], n_experiments: int = 5) -> Dict:
        """è¿è¡Œç®—æ³•æ¯”è¾ƒå®éªŒ"""
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹å…¬å¹³æ¯”è¾ƒå®éªŒ")
        self.logger.info("=" * 80)
        self.logger.info(f"å‚ä¸æ¯”è¾ƒçš„ç®—æ³•: {algorithms}")
        self.logger.info(f"æ¯ä¸ªç®—æ³•è¿è¡Œæ¬¡æ•°: {n_experiments}")
        if self.fixed_workflow_instances is not None:
            self.logger.info(f"ä½¿ç”¨å›ºå®šçš„å·¥ä½œæµå®ä¾‹é›†: {len(self.fixed_workflow_instances)} ä¸ªå·¥ä½œæµ")
        else:
            self.logger.warning("å›ºå®šå·¥ä½œæµå®ä¾‹é›†æœªåˆå§‹åŒ–")
        
        results = {}
        
        for algorithm_name in algorithms:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"è¿è¡Œç®—æ³•: {algorithm_name}")
            self.logger.info(f"{'='*60}")
            
            algorithm_results = []
            
            for exp in range(n_experiments):
                self.logger.info(f"\n--- å®éªŒ {exp + 1}/{n_experiments} ---")
                
                try:
                    # ä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºæ–°çš„æ¨¡æ‹Ÿå™¨ï¼ˆä½†ä½¿ç”¨ç›¸åŒçš„å·¥ä½œæµå®ä¾‹ï¼‰
                    simulator = self.create_simulator_with_fixed_instances()
                    
                    # è¿è¡Œç®—æ³•
                    if algorithm_name == "FE_IDDQN":
                        result = self._run_fe_iddqn(simulator, exp)
                    elif algorithm_name in ["FIFO", "SJF", "HEFT"]:
                        result = self._run_traditional_scheduler(simulator, algorithm_name, exp)
                    elif algorithm_name in ["DQN", "DDQN", "BF_DDQN"]:
                        result = self._run_rl_baseline(simulator, algorithm_name, exp)
                    elif algorithm_name in ["GA", "PSO", "ACO"]:
                        result = self._run_meta_heuristic(simulator, algorithm_name, exp)
                    else:
                        self.logger.warning(f"æœªçŸ¥ç®—æ³•: {algorithm_name}")
                        continue
                    
                    algorithm_results.append(result)
                    self.logger.info(f"å®éªŒ {exp + 1} å®Œæˆ: Makespan={result.get('makespan', 'N/A'):.2f}, "
                                  f"èµ„æºåˆ©ç”¨ç‡={result.get('resource_utilization', 'N/A'):.2f}")
                    
                except Exception as e:
                    self.logger.error(f"ç®—æ³• {algorithm_name} å®éªŒ {exp + 1} å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            # è®¡ç®—ç»Ÿè®¡ç»“æœ
            if algorithm_results:
                results[algorithm_name] = self._calculate_statistics(algorithm_results)
                self.logger.info(f"\nç®—æ³• {algorithm_name} ç»Ÿè®¡ç»“æœ:")
                self.logger.info(f"  å¹³å‡ Makespan: {results[algorithm_name]['avg_makespan']:.2f} Â± {results[algorithm_name]['std_makespan']:.2f}")
                self.logger.info(f"  å¹³å‡èµ„æºåˆ©ç”¨ç‡: {results[algorithm_name]['avg_resource_utilization']:.2f} Â± {results[algorithm_name]['std_resource_utilization']:.2f}")
                self.logger.info(f"  å¹³å‡å¥–åŠ±: {results[algorithm_name]['avg_reward']:.2f} Â± {results[algorithm_name]['std_reward']:.2f}")
            else:
                self.logger.error(f"ç®—æ³• {algorithm_name} æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœï¼")
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        self._generate_comparison_report(results)
        
        return results
    
    def _run_fe_iddqn(self, simulator: HistoricalReplaySimulator, exp_id: int) -> Dict:
        """è¿è¡ŒFE-IDDQNç®—æ³•"""
        # è·å–ç‰¹å¾ç»´åº¦
        task_features, resource_features = simulator.get_state()
        task_input_dim = task_features.shape[-1] if task_features.size > 0 else 16
        resource_input_dim = resource_features.shape[-1] if resource_features.size > 0 else 7
        action_dim = simulator.num_resources
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = FE_IDDQN(task_input_dim, resource_input_dim, action_dim, self.device)
        
        # è·å–ç®—æ³•å‚æ•°
        algorithm_params = Hyperparameters.get_algorithm_params('FE_IDDQN')
        
        # è®­ç»ƒè¿‡ç¨‹
        episode_rewards = []
        episode_makespans = []
        
        for episode in range(algorithm_params.get('num_episodes', 50)):
            simulator.reset()
            episode_reward = 0
            step_count = 0
            
            max_steps = algorithm_params.get('max_steps_per_episode', 1000)
            
            while not simulator.is_done() and step_count < max_steps:
                state = simulator.get_state()
                action = agent.select_action(state[0], state[1])
                next_state, reward, done, info = simulator.step(action)
                
                agent.store_experience(state, action, reward, next_state, done)
                
                if step_count % algorithm_params.get('train_freq', 4) == 0:
                    agent.train()
                
                episode_reward += reward
                step_count += 1
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
            episode_makespans.append(simulator.get_makespan())
            
            # æ›´æ–°ç½‘ç»œ
            if episode % algorithm_params.get('target_update_frequency', 10) == 0:
                agent.update_target_network()
            
            agent.update_exploration_params()
        
        # æœ€ç»ˆè¯„ä¼°
        final_metrics = {
            'makespan': simulator.get_makespan(),
            'resource_utilization': simulator.get_resource_utilization(),
            'average_reward': np.mean(episode_rewards),
            'final_episode_reward': episode_rewards[-1] if episode_rewards else 0,
            'training_losses': agent.get_training_stats().get('training_losses', []),
            'experiment_id': exp_id,
            'algorithm': 'FE_IDDQN'
        }
        
        return final_metrics
    
    def _run_rl_baseline(self, simulator: HistoricalReplaySimulator, algorithm_name: str, exp_id: int) -> Dict:
        """è¿è¡ŒRLåŸºçº¿ç®—æ³•"""
        # ç®€åŒ–å®ç°ï¼Œä½¿ç”¨éšæœºè°ƒåº¦
        schedule_result = simulator.simulate_random_schedule(algorithm_name)
        
        return {
            'makespan': schedule_result.get('makespan', 0),
            'resource_utilization': schedule_result.get('resource_utilization', 0),
            'average_reward': 0,
            'final_episode_reward': 0,
            'experiment_id': exp_id,
            'algorithm': algorithm_name
        }
    
    def _run_traditional_scheduler(self, simulator: HistoricalReplaySimulator, algorithm_name: str, exp_id: int) -> Dict:
        """è¿è¡Œä¼ ç»Ÿè°ƒåº¦ç®—æ³•"""
        if algorithm_name == "FIFO":
            scheduler = FIFOScheduler()
        elif algorithm_name == "SJF":
            scheduler = SJFScheduler()
        elif algorithm_name == "HEFT":
            scheduler = HEFTScheduler()
        else:
            raise ValueError(f"æœªçŸ¥çš„ä¼ ç»Ÿè°ƒåº¦ç®—æ³•: {algorithm_name}")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        self.logger.info(f"  è°ƒè¯•ä¿¡æ¯ - ç®—æ³• {algorithm_name}:")
        self.logger.info(f"    ä»»åŠ¡æ•°é‡: {len(simulator.tasks)}")
        self.logger.info(f"    èµ„æºæ•°é‡: {len(simulator.resources)}")
        self.logger.info(f"    ä¾èµ–å…³ç³»æ•°é‡: {len(simulator.dependencies)}")
        
        # æ˜¾ç¤ºä»»åŠ¡IDå’Œä¾èµ–å…³ç³»
        if simulator.tasks:
            task_ids = [task['id'] for task in simulator.tasks]
            self.logger.info(f"    ä»»åŠ¡IDåˆ—è¡¨: {task_ids}")
        
        if simulator.dependencies:
            dep_info = [(dep.get('pre_task'), dep.get('post_task')) for dep in simulator.dependencies]
            self.logger.info(f"    ä¾èµ–å…³ç³»: {dep_info}")
        
        # è½¬æ¢ä¾èµ–å…³ç³»æ ¼å¼
        dependencies = [(dep['pre_task'], dep['post_task']) 
                       for dep in simulator.dependencies 
                       if dep['pre_task'] is not None and dep['post_task'] is not None]
        
        self.logger.info(f"    è½¬æ¢åçš„ä¾èµ–å…³ç³»: {dependencies}")
        
        # éªŒè¯ä¾èµ–å…³ç³»ä¸­çš„ä»»åŠ¡IDæ˜¯å¦éƒ½å­˜åœ¨äºä»»åŠ¡åˆ—è¡¨ä¸­
        if simulator.tasks:
            available_task_ids = set(task['id'] for task in simulator.tasks)
            dependency_task_ids = set()
            for pre_task, post_task in dependencies:
                if pre_task is not None:
                    dependency_task_ids.add(pre_task)
                if post_task is not None:
                    dependency_task_ids.add(post_task)
            
            missing_task_ids = dependency_task_ids - available_task_ids
            if missing_task_ids:
                self.logger.warning(f"    è­¦å‘Š: ä¾èµ–å…³ç³»ä¸­çš„ä»»åŠ¡IDåœ¨ä»»åŠ¡åˆ—è¡¨ä¸­ä¸å­˜åœ¨: {missing_task_ids}")
                self.logger.warning(f"    è¿™å¯èƒ½å¯¼è‡´è°ƒåº¦å¤±è´¥")
        
        schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if 'error' in schedule_result:
            self.logger.error(f"ç®—æ³• {algorithm_name} è°ƒåº¦å¤±è´¥: {schedule_result['error']}")
            return {
                'makespan': float('inf'),
                'resource_utilization': 0,
                'average_reward': -1000,
                'final_episode_reward': -1000,
                'experiment_id': exp_id,
                'algorithm': algorithm_name,
                'error': schedule_result['error']
            }
        
        # è®¡ç®—makespanå’Œèµ„æºåˆ©ç”¨ç‡
        makespan = schedule_result.get('makespan', 0)
        resource_utilization = schedule_result.get('resource_utilization', 0)
        
        # å¦‚æœæ²¡æœ‰è¿™äº›å­—æ®µï¼Œå°è¯•è®¡ç®—
        if makespan == 0 and 'task_end_times' in schedule_result:
            if schedule_result['task_end_times']:
                makespan = max(schedule_result['task_end_times'].values())
        
        if resource_utilization == 0 and makespan > 0:
            total_work = sum(task['duration'] for task in simulator.tasks)
            total_capacity = makespan * len(simulator.resources)
            resource_utilization = total_work / total_capacity if total_capacity > 0 else 0
        
        return {
            'makespan': makespan,
            'resource_utilization': resource_utilization,
            'average_reward': 0,
            'final_episode_reward': 0,
            'experiment_id': exp_id,
            'algorithm': algorithm_name
        }
    
    def _run_meta_heuristic(self, simulator: HistoricalReplaySimulator, algorithm_name: str, exp_id: int) -> Dict:
        """è¿è¡Œå…ƒå¯å‘å¼ç®—æ³•"""
        if algorithm_name == "GA":
            scheduler = GAScheduler()
        elif algorithm_name == "PSO":
            scheduler = PSOScheduler()
        elif algorithm_name == "ACO":
            scheduler = ACOScheduler()
        else:
            raise ValueError(f"æœªçŸ¥çš„å…ƒå¯å‘å¼ç®—æ³•: {algorithm_name}")
        
        # è½¬æ¢ä¾èµ–å…³ç³»æ ¼å¼
        dependencies = [(dep['pre_task'], dep['post_task']) 
                       for dep in simulator.dependencies 
                       if dep['pre_task'] is not None and dep['post_task'] is not None]
        
        schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
        
        return {
            'makespan': schedule_result.get('makespan', 0),
            'resource_utilization': schedule_result.get('resource_utilization', 0),
            'average_reward': 0,
            'final_episode_reward': 0,
            'experiment_id': exp_id,
            'algorithm': algorithm_name
        }
    
    def _calculate_statistics(self, results: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ç»“æœ"""
        makespans = [r['makespan'] for r in results if isinstance(r.get('makespan'), (int, float))]
        resource_utilizations = [r['resource_utilization'] for r in results if isinstance(r.get('resource_utilization'), (int, float))]
        rewards = [r['average_reward'] for r in results if isinstance(r.get('average_reward'), (int, float))]
        
        return {
            'avg_makespan': np.mean(makespans) if makespans else 0,
            'std_makespan': np.std(makespans) if makespans else 0,
            'avg_resource_utilization': np.mean(resource_utilizations) if resource_utilizations else 0,
            'std_resource_utilization': np.std(resource_utilizations) if resource_utilizations else 0,
            'avg_reward': np.mean(rewards) if rewards else 0,
            'std_reward': np.std(rewards) if rewards else 0,
            'n_experiments': len(results),
            'raw_results': results
        }
    
    def _generate_comparison_report(self, results: Dict):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        self.logger.info("\n" + "="*80)
        self.logger.info("ç®—æ³•æ€§èƒ½æ¯”è¾ƒæŠ¥å‘Š")
        self.logger.info("="*80)
        
        # æŒ‰makespanæ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        sorted_algorithms = sorted(results.keys(), 
                                 key=lambda x: results[x]['avg_makespan'])
        
        self.logger.info("\nğŸ† ç®—æ³•æ€§èƒ½æ’å (æŒ‰makespanæ’åºï¼Œè¶Šå°è¶Šå¥½):")
        for i, algorithm in enumerate(sorted_algorithms, 1):
            result = results[algorithm]
            self.logger.info(f"  {i}. {algorithm}:")
            self.logger.info(f"    Makespan: {result['avg_makespan']:.2f} Â± {result['std_makespan']:.2f}")
            self.logger.info(f"    èµ„æºåˆ©ç”¨ç‡: {result['avg_resource_utilization']:.2f} Â± {result['std_resource_utilization']:.2f}")
            self.logger.info(f"    å¹³å‡å¥–åŠ±: {result['avg_reward']:.2f} Â± {result['std_reward']:.2f}")
            self.logger.info(f"    å®éªŒæ¬¡æ•°: {result['n_experiments']}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        report_file = os.path.join(self.output_dir, 'algorithm_comparison_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"\nè¯¦ç»†æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # ç”Ÿæˆæ€§èƒ½æå‡åˆ†æ
        if 'FE_IDDQN' in results:
            fe_result = results['FE_IDDQN']
            self.logger.info(f"\nğŸš€ FE-IDDQN ç®—æ³•æ€§èƒ½åˆ†æ:")
            
            for algorithm, result in results.items():
                if algorithm != 'FE_IDDQN':
                    makespan_improvement = ((result['avg_makespan'] - fe_result['avg_makespan']) / result['avg_makespan']) * 100
                    utilization_improvement = ((fe_result['avg_resource_utilization'] - result['avg_resource_utilization']) / result['avg_resource_utilization']) * 100
                    
                    self.logger.info(f"  ç›¸æ¯” {algorithm}:")
                    self.logger.info(f"    Makespan æå‡: {makespan_improvement:+.1f}%")
                    self.logger.info(f"    èµ„æºåˆ©ç”¨ç‡æå‡: {utilization_improvement:+.1f}%")
        
        self.logger.info("="*80)
