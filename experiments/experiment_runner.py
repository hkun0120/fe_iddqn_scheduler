import logging
import pandas as pd
import numpy as np
import torch
from typing import Dict, List, Tuple, Any
from config.config import Config
from config.hyperparameters import Hyperparameters
from models.fe_iddqn import FE_IDDQN
from baselines.traditional_schedulers import FIFOScheduler, SJFScheduler, HEFTScheduler
from baselines.rl_baselines import DQNScheduler, DDQNScheduler, BF_DDQNScheduler
from baselines.meta_heuristics import GAScheduler, PSOScheduler, ACOScheduler
from environment.workflow_simulator import WorkflowSimulator
from environment.historical_replay_simulator import HistoricalReplaySimulator
from evaluation.metrics import Evaluator


class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨ï¼Œè´Ÿè´£è¿è¡Œä¸åŒç®—æ³•å¹¶æ”¶é›†ç»“æœ"""

    def __init__(self, data: Dict[str, pd.DataFrame], features: pd.DataFrame,
                 output_dir: str, n_experiments: int = Config.N_EXPERIMENTS):
        self.logger = logging.getLogger(__name__)
        self.data = data
        self.features = features
        self.output_dir = output_dir
        self.n_experiments = n_experiments
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.evaluator = Evaluator()

    def _build_tasks_from_data(self) -> List[Dict]:
        """ä»çœŸå®æ•°æ®æ„å»ºä»»åŠ¡åˆ—è¡¨"""
        tasks = []
        
        # è·å–ä»»åŠ¡å®ä¾‹æ•°æ®
        task_instances = self.data['task_instance']
        task_definitions = self.data['task_definition']
        
        # åˆå¹¶ä»»åŠ¡å®šä¹‰å’Œå®ä¾‹
        merged_tasks = pd.merge(
            task_instances,
            task_definitions,
            left_on=['task_code', 'task_definition_version'],
            right_on=['code', 'version'],
            how='left',
            suffixes=('_instance', '_definition')
        )
        
        # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„ä»»åŠ¡ï¼ˆæœ‰å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼‰
        valid_tasks = merged_tasks[
            (merged_tasks['start_time'].notna()) & 
            (merged_tasks['end_time'].notna()) &
            (merged_tasks['state'] == 7)  # åªé€‰æ‹©æˆåŠŸçš„ä»»åŠ¡
        ].copy()
        
        # è®¡ç®—ä»»åŠ¡æŒç»­æ—¶é—´
        valid_tasks['duration'] = (
            pd.to_datetime(valid_tasks['end_time']) - 
            pd.to_datetime(valid_tasks['start_time'])
        ).dt.total_seconds()
        
        # æ„å»ºä»»åŠ¡å­—å…¸
        for idx, task in valid_tasks.iterrows():
            # ä¼°ç®—èµ„æºéœ€æ±‚ï¼ˆåŸºäºä»»åŠ¡ç±»å‹å’Œå†å²æ•°æ®ï¼‰
            cpu_req = self._estimate_cpu_requirement(task)
            memory_req = self._estimate_memory_requirement(task)
            
            tasks.append({
                "id": task['id_instance'],
                "name": task['name_instance'],
                "task_type": task['task_type'],
                "duration": task['duration'],
                "cpu_req": cpu_req,
                "memory_req": memory_req,
                "submit_time": task['submit_time'],
                "start_time": task['start_time'],
                "end_time": task['end_time'],
                "host": task['host'],
                "worker_group": task['worker_group'],
                "priority": task['task_instance_priority'],
                "retry_times": task['retry_times'],
                "process_instance_id": task['process_instance_id']
            })
        
        return tasks

    def _build_resources_from_data(self) -> List[Dict]:
        """ä»çœŸå®æ•°æ®æ„å»ºèµ„æºåˆ—è¡¨"""
        resources = []
        
        # ä»ä»»åŠ¡å®ä¾‹ä¸­æå–ä¸»æœºä¿¡æ¯
        task_instances = self.data['task_instance']
        hosts = task_instances['host'].dropna().unique()
        
        # åŸºäºå†å²æ•°æ®ä¼°ç®—æ¯ä¸ªä¸»æœºçš„èµ„æºå®¹é‡
        for host in hosts:
            host_tasks = task_instances[task_instances['host'] == host]
            
            # ä¼°ç®—CPUå’Œå†…å­˜å®¹é‡ï¼ˆåŸºäºå†å²ä»»åŠ¡çš„æœ€å¤§éœ€æ±‚ï¼‰
            cpu_capacity = self._estimate_host_cpu_capacity(host_tasks)
            memory_capacity = self._estimate_host_memory_capacity(host_tasks)
            
            resources.append({
                "id": host,
                "host": host,
                "cpu_capacity": cpu_capacity,
                "memory_capacity": memory_capacity,
                "worker_group": host_tasks['worker_group'].iloc[0] if not host_tasks.empty else 'default'
            })
        
        return resources

    def _build_dependencies_from_data(self) -> List[Tuple[int, int]]:
        """ä»çœŸå®æ•°æ®æ„å»ºä»»åŠ¡ä¾èµ–å…³ç³»"""
        dependencies = []
        
        # è·å–è¿›ç¨‹ä»»åŠ¡å…³ç³»æ•°æ®
        process_task_relations = self.data['process_task_relation']
        task_instances = self.data['task_instance']
        
        # åŸºäºè¿›ç¨‹å®ä¾‹IDå’Œæ—¶é—´å…³ç³»æ¨æ–­ä¾èµ–
        process_instances = self.data['process_instance']
        
        for _, relation in process_task_relations.iterrows():
            # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡å®ä¾‹
            pre_tasks = task_instances[
                (task_instances['task_code'] == relation['pre_task_code']) &
                (task_instances['process_instance_id'] == relation['process_instance_id'])
            ]
            
            post_tasks = task_instances[
                (task_instances['task_code'] == relation['post_task_code']) &
                (task_instances['process_instance_id'] == relation['process_instance_id'])
            ]
            
            for _, pre_task in pre_tasks.iterrows():
                for _, post_task in post_tasks.iterrows():
                    dependencies.append((pre_task['id'], post_task['id']))
        
        return dependencies

    def _estimate_cpu_requirement(self, task: pd.Series) -> int:
        """ä¼°ç®—ä»»åŠ¡çš„CPUéœ€æ±‚"""
        # åŸºäºä»»åŠ¡ç±»å‹å’Œå†å²æ•°æ®ä¼°ç®—
        task_type = task['task_type']
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®åŸºç¡€CPUéœ€æ±‚
        base_cpu = {
            'SQL': 2,
            'SHELL': 1,
            'PYTHON': 2,
            'JAVA': 3,
            'SPARK': 4,
            'FLINK': 4,
            'HTTP': 1
        }.get(task_type, 1)
        
        # æ ¹æ®æŒç»­æ—¶é—´è°ƒæ•´
        duration = task.get('duration', 60)
        if duration > 300:  # è¶…è¿‡5åˆ†é’Ÿçš„ä»»åŠ¡
            base_cpu = min(base_cpu + 1, 8)
        
        return base_cpu

    def _estimate_memory_requirement(self, task: pd.Series) -> int:
        """ä¼°ç®—ä»»åŠ¡çš„å†…å­˜éœ€æ±‚"""
        task_type = task['task_type']
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®åŸºç¡€å†…å­˜éœ€æ±‚ï¼ˆGBï¼‰
        base_memory = {
            'SQL': 1,
            'SHELL': 0.5,
            'PYTHON': 2,
            'JAVA': 4,
            'SPARK': 8,
            'FLINK': 8,
            'HTTP': 1
        }.get(task_type, 1)
        
        return base_memory

    def _estimate_host_cpu_capacity(self, host_tasks: pd.DataFrame) -> int:
        """ä¼°ç®—ä¸»æœºçš„CPUå®¹é‡"""
        # åŸºäºå†å²ä»»åŠ¡çš„æœ€å¤§å¹¶å‘æ•°ä¼°ç®—
        if host_tasks.empty:
            return 8  # é»˜è®¤å€¼
        
        # è®¡ç®—åŒæ—¶è¿è¡Œçš„ä»»åŠ¡æ•°
        host_tasks['start_time'] = pd.to_datetime(host_tasks['start_time'])
        host_tasks['end_time'] = pd.to_datetime(host_tasks['end_time'])
        
        # ç®€å•çš„ä¼°ç®—ï¼šåŸºäºä»»åŠ¡æ•°é‡
        return min(max(len(host_tasks) // 10 + 4, 4), 16)

    def _estimate_host_memory_capacity(self, host_tasks: pd.DataFrame) -> int:
        """ä¼°ç®—ä¸»æœºçš„å†…å­˜å®¹é‡"""
        if host_tasks.empty:
            return 16  # é»˜è®¤å€¼
        
        # åŸºäºå†å²ä»»åŠ¡çš„å†…å­˜éœ€æ±‚ä¼°ç®—
        return min(max(len(host_tasks) // 5 + 8, 8), 64)

    def _initialize_simulator(self) -> WorkflowSimulator:
        """ä»çœŸå®æ•°æ®åˆå§‹åŒ–ä»¿çœŸç¯å¢ƒ"""
        self.logger.info("Building simulator from real data...")
        
        # ä»çœŸå®æ•°æ®æ„å»ºä»»åŠ¡ã€èµ„æºå’Œä¾èµ–å…³ç³»
        tasks = self._build_tasks_from_data()
        resources = self._build_resources_from_data()
        dependencies = self._build_dependencies_from_data()
        
        self.logger.info(f"Built simulator with {len(tasks)} tasks, {len(resources)} resources, {len(dependencies)} dependencies")
        
        return WorkflowSimulator(tasks, resources, dependencies)

    def _create_historical_replay_simulator(self) -> WorkflowSimulator:
        """åˆ›å»ºåŸºäºå†å²æ•°æ®é‡æ”¾çš„ä»¿çœŸå™¨"""
        self.logger.info("Creating historical replay simulator...")
        
        # è·å–æˆåŠŸçš„è¿›ç¨‹å®ä¾‹
        process_instances = self.data['process_instance']
        successful_processes = process_instances[process_instances['state'] == 7]
        
        # ä¸ºæ¯ä¸ªæˆåŠŸçš„è¿›ç¨‹å®ä¾‹åˆ›å»ºé‡æ”¾ä»»åŠ¡
        replay_tasks = []
        replay_resources = []
        replay_dependencies = []
        
        for _, process in successful_processes.iterrows():
            # è·å–è¯¥è¿›ç¨‹çš„æ‰€æœ‰ä»»åŠ¡å®ä¾‹
            process_tasks = self.data['task_instance'][
                self.data['task_instance']['process_instance_id'] == process['id']
            ]
            
            # æŒ‰å¼€å§‹æ—¶é—´æ’åº
            process_tasks = process_tasks.sort_values('start_time')
            
            # åˆ›å»ºé‡æ”¾ä»»åŠ¡
            for idx, task in process_tasks.iterrows():
                if task['state'] == 7:  # åªé‡æ”¾æˆåŠŸçš„ä»»åŠ¡
                    duration = (
                        pd.to_datetime(task['end_time']) - 
                        pd.to_datetime(task['start_time'])
                    ).total_seconds()
                    
                    replay_tasks.append({
                        "id": f"{process['id']}_{task['id']}",
                        "name": task['name'],
                        "task_type": task['task_type'],
                        "duration": duration,
                        "cpu_req": self._estimate_cpu_requirement(task),
                        "memory_req": self._estimate_memory_requirement(task),
                        "submit_time": task['submit_time'],
                        "start_time": task['start_time'],
                        "end_time": task['end_time'],
                        "host": task['host'],
                        "process_instance_id": process['id']
                    })
        
        # åˆ›å»ºèµ„æºï¼ˆåŸºäºå®é™…ä¸»æœºï¼‰
        hosts = pd.concat([self.data['task_instance']['host'], 
                          self.data['process_instance']['host']]).dropna().unique()
        
        for host in hosts:
            replay_resources.append({
                "id": host,
                "host": host,
                "cpu_capacity": self._estimate_host_cpu_capacity(
                    self.data['task_instance'][self.data['task_instance']['host'] == host]
                ),
                "memory_capacity": self._estimate_host_memory_capacity(
                    self.data['task_instance'][self.data['task_instance']['host'] == host]
                )
            })
        
        self.logger.info(f"Created replay simulator with {len(replay_tasks)} tasks, {len(replay_resources)} resources")
        
        return WorkflowSimulator(replay_tasks, replay_resources, replay_dependencies)

    def run_algorithm(self, algorithm_name: str, use_historical_replay: bool = True) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªç®—æ³•çš„å®éªŒ"""
        self.logger.info(f"Running experiment for algorithm: {algorithm_name}")

        results = []
        for i in range(self.n_experiments):
            self.logger.info(f"  Experiment {i + 1}/{self.n_experiments}")
            
            # é€‰æ‹©ä»¿çœŸå™¨ç±»å‹
            if use_historical_replay:
                # ä½¿ç”¨å†å²é‡æ”¾ä»¿çœŸå™¨
                simulator = HistoricalReplaySimulator(
                    self.data['process_instance'],
                    self.data['task_instance'],
                    self.data['task_definition'],
                    self.data['process_task_relation']
                )
            else:
                # ä½¿ç”¨ä¼ ç»Ÿä»¿çœŸå™¨
                simulator = self._initialize_simulator()
            
            # è·å–ç®—æ³•å‚æ•°
            algorithm_params = Hyperparameters.get_algorithm_params(algorithm_name)

            if algorithm_name == "FE_IDDQN":
                # ä»çœŸå®æ•°æ®ä¸­æå–ç‰¹å¾ç»´åº¦
                task_features, resource_features = simulator.get_state()
                
                # æ‰“å°æ•°æ®é›†ä¿¡æ¯
                self.logger.info("=" * 60)
                self.logger.info("FE-IDDQN ç®—æ³•æ•°æ®é›†ä¿¡æ¯:")
                self.logger.info("=" * 60)
                self.logger.info(f"ä»»åŠ¡ç‰¹å¾ç»´åº¦: {task_features.shape}")
                self.logger.info(f"èµ„æºç‰¹å¾ç»´åº¦: {resource_features.shape}")
                self.logger.info(f"å¯ç”¨èµ„æºæ•°é‡: {simulator.num_resources}")
                
                # æ‰“å°ä»»åŠ¡ç‰¹å¾ç¤ºä¾‹
                if task_features.size > 0:
                    self.logger.info(f"\nä»»åŠ¡ç‰¹å¾ç¤ºä¾‹ (å‰3ä¸ªä»»åŠ¡):")
                    for i in range(min(3, task_features.shape[1])):
                        task_feat = task_features[0, i, :]
                        self.logger.info(f"  ä»»åŠ¡ {i}: {task_feat[:10]}... (å…±{len(task_feat)}ä¸ªç‰¹å¾)")
                
                # æ‰“å°èµ„æºç‰¹å¾ç¤ºä¾‹
                if resource_features.size > 0:
                    self.logger.info(f"\nèµ„æºç‰¹å¾ç¤ºä¾‹ (å‰3ä¸ªèµ„æº):")
                    for i in range(min(3, resource_features.shape[1])):
                        resource_feat = resource_features[0, i, :]
                        self.logger.info(f"  èµ„æº {i}: {resource_feat} (å…±{len(resource_feat)}ä¸ªç‰¹å¾)")
                
                # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
                self.logger.info(f"\næ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
                self.logger.info(f"  æ€»è¿›ç¨‹æ•°: {len(self.data['process_instance'])}")
                self.logger.info(f"  æ€»ä»»åŠ¡æ•°: {len(self.data['task_instance'])}")
                self.logger.info(f"  ä»»åŠ¡å®šä¹‰æ•°: {len(self.data['task_definition'])}")
                self.logger.info(f"  ä»»åŠ¡å…³ç³»æ•°: {len(self.data['process_task_relation'])}")
                
                # ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
                task_types = self.data['task_instance']['task_type'].value_counts()
                self.logger.info(f"  ä»»åŠ¡ç±»å‹åˆ†å¸ƒ:")
                for task_type, count in task_types.head(10).items():
                    self.logger.info(f"    {task_type}: {count}")
                
                self.logger.info("=" * 60)
                
                # ä»»åŠ¡ç‰¹å¾ç»´åº¦æ˜¯æœ€åä¸€ä¸ªç»´åº¦ï¼ˆç‰¹å¾æ•°é‡ï¼‰
                task_input_dim = task_features.shape[-1] if task_features.size > 0 else 16
                resource_input_dim = resource_features.shape[-1] if resource_features.size > 0 else 7
                action_dim = simulator.num_resources

                agent = FE_IDDQN(task_input_dim, resource_input_dim, action_dim, self.device)
                
                # å®é™…è®­ç»ƒè¿‡ç¨‹
                episode_rewards = []
                episode_makespans = []
                
                for episode in range(algorithm_params.get('num_episodes', 100)):
                    simulator.reset()
                    episode_reward = 0
                    step_count = 0
                    
                    # å¢åŠ æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°ï¼Œè®©ç®—æ³•èƒ½å¤Ÿè°ƒåº¦æ›´å¤šä»»åŠ¡
                    max_steps = algorithm_params.get('max_steps_per_episode', 1000)
                    max_steps = max(max_steps, 2000)  # ç¡®ä¿è‡³å°‘æœ‰2000æ­¥
                    
                    while not simulator.is_done() and step_count < max_steps:
                        state = simulator.get_state()
                        task_features, resource_features = state
                        
                        # é€‰æ‹©åŠ¨ä½œ
                        action = agent.select_action(task_features, resource_features)
                        
                        # æ‰§è¡ŒåŠ¨ä½œ
                        next_state, reward, done, info = simulator.step(action)
                        
                        # æ·»åŠ stepæ‰§è¡Œåçš„è°ƒè¯•ä¿¡æ¯
                        if step_count % 50 == 0:  # æ¯50æ­¥æ£€æŸ¥ä¸€æ¬¡
                            self.logger.info(f"        Stepæ‰§è¡Œç»“æœ: reward={reward:.2f}, done={done}, info={info}")
                            
                            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦çœŸçš„è¢«è°ƒåº¦äº†
                            if 'task_scheduled' in info and info['task_scheduled']:
                                self.logger.info(f"        âœ… ä»»åŠ¡æˆåŠŸè°ƒåº¦: {info.get('task_name', 'Unknown')} -> {info.get('host', 'Unknown')}")
                            else:
                                self.logger.warning(f"        âš ï¸  ä»»åŠ¡è°ƒåº¦å¤±è´¥æˆ–æœªè°ƒåº¦")
                        
                        # å­˜å‚¨ç»éªŒ
                        agent.store_experience(state, action, reward, next_state, done)
                        
                        # è®­ç»ƒç½‘ç»œ
                        if step_count % algorithm_params.get('train_freq', 4) == 0:
                            loss = agent.train()
                        
                        episode_reward += reward
                        step_count += 1
                        
                        # ä¿®å¤ï¼šå¦‚æœepisodeå®Œæˆï¼Œç«‹å³é€€å‡ºå¾ªç¯
                        if simulator.is_done():
                            self.logger.info(f"        ğŸ¯ Episodeåœ¨ç¬¬{step_count}æ­¥å®Œæˆï¼Œé€€å‡ºè®­ç»ƒå¾ªç¯")
                            break
                        
                        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                        if step_count % 50 == 0:  # æ›´é¢‘ç¹çš„æ—¥å¿—è¾“å‡º
                            process_info = simulator.get_current_process_info()
                            if process_info:
                                self.logger.info(f"      Step {step_count}: Process {process_info['process_id']}, "
                                               f"Completed: {process_info['completed_tasks']}/{process_info['total_tasks']}")
                                
                                # æ£€æŸ¥å½“å‰ä»»åŠ¡çŠ¶æ€
                                if hasattr(simulator, 'current_task_idx') and hasattr(simulator, 'current_process_tasks'):
                                    if simulator.current_task_idx < len(simulator.current_process_tasks):
                                        current_task = simulator.current_process_tasks.iloc[simulator.current_task_idx]
                                        self.logger.info(f"        å½“å‰ä»»åŠ¡: {current_task.get('name', 'Unknown')} "
                                                       f"(ID: {current_task.get('id', 'Unknown')})")
                                        
                                        # æ£€æŸ¥ä»»åŠ¡èµ„æºéœ€æ±‚
                                        if hasattr(simulator, '_estimate_task_cpu_requirement'):
                                            cpu_req = simulator._estimate_task_cpu_requirement(current_task)
                                            memory_req = simulator._estimate_task_memory_requirement(current_task)
                                            self.logger.info(f"        èµ„æºéœ€æ±‚: CPU {cpu_req:.1f}, Memory {memory_req:.1f}")
                                    
                                # æ£€æŸ¥èµ„æºçŠ¶æ€
                                if hasattr(simulator, 'available_resources'):
                                    self.logger.info(f"        èµ„æºçŠ¶æ€:")
                                    for host, resource in simulator.available_resources.items():
                                        cpu_used = resource.get('cpu_used', 0)
                                        cpu_capacity = resource.get('cpu_capacity', 0)
                                        memory_used = resource.get('memory_used', 0)
                                        memory_capacity = resource.get('memory_capacity', 0)
                                        self.logger.info(f"          {host}: CPU {cpu_used:.1f}/{cpu_capacity:.1f}, "
                                                       f"Memory {memory_used:.1f}/{memory_capacity:.1f}")
                                
                                # æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€
                                if hasattr(simulator, 'completed_tasks'):
                                    self.logger.info(f"        å·²å®Œæˆä»»åŠ¡æ•°: {len(simulator.completed_tasks)}")
                                
                                # æ£€æŸ¥è¿›ç¨‹å’Œä»»åŠ¡ç´¢å¼•
                                if hasattr(simulator, 'current_process_idx'):
                                    self.logger.info(f"        å½“å‰è¿›ç¨‹ç´¢å¼•: {simulator.current_process_idx}")
                                if hasattr(simulator, 'current_task_idx'):
                                    self.logger.info(f"        å½“å‰ä»»åŠ¡ç´¢å¼•: {simulator.current_task_idx}")
                                
                                # æ£€æŸ¥æ˜¯å¦å¡ä½
                                if step_count > 200 and process_info['completed_tasks'] == 0:
                                    self.logger.warning(f"        âš ï¸  è­¦å‘Š: 200æ­¥åä»æ— ä»»åŠ¡å®Œæˆï¼Œå¯èƒ½å­˜åœ¨å¡ä½é—®é¢˜ï¼")
                                
                                if step_count > 500 and process_info['completed_tasks'] == 0:
                                    self.logger.error(f"        âŒ é”™è¯¯: 500æ­¥åä»æ— ä»»åŠ¡å®Œæˆï¼Œå¼ºåˆ¶æ£€æŸ¥é—®é¢˜ï¼")
                                    # å¼ºåˆ¶æ£€æŸ¥å½“å‰çŠ¶æ€
                                    if hasattr(simulator, 'ready_tasks'):
                                        self.logger.info(f"          å¯è°ƒåº¦ä»»åŠ¡: {simulator.ready_tasks}")
                                    if hasattr(simulator, 'is_done'):
                                        self.logger.info(f"          æ˜¯å¦å®Œæˆ: {simulator.is_done()}")
                        
                        # æ¯100æ­¥çš„åŸæœ‰æ—¥å¿—
                        if step_count % 100 == 0:
                            process_info = simulator.get_current_process_info()
                            if process_info:
                                self.logger.info(f"      Step {step_count}: Process {process_info['process_id']}, "
                                               f"Completed: {process_info['completed_tasks']}/{process_info['total_tasks']}")
                        
                        # æ£€æŸ¥æ˜¯å¦å¡ä½ï¼Œå¼ºåˆ¶åˆ‡æ¢è¿›ç¨‹
                        if step_count > 1000 and process_info and process_info['completed_tasks'] == 0:
                            self.logger.error(f"        ğŸš¨ ä¸¥é‡é”™è¯¯: 1000æ­¥åä»æ— ä»»åŠ¡å®Œæˆï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªè¿›ç¨‹ï¼")
                            if hasattr(simulator, 'current_process_idx') and hasattr(simulator, '_load_current_process'):
                                simulator.current_process_idx += 1
                                if simulator._load_current_process():
                                    self.logger.info(f"        å¼ºåˆ¶åˆ‡æ¢åˆ°è¿›ç¨‹ç´¢å¼•: {simulator.current_process_idx}")
                                    step_count = 0  # é‡ç½®æ­¥æ•°
                                else:
                                    self.logger.info(f"        æ‰€æœ‰è¿›ç¨‹å·²å®Œæˆï¼Œé€€å‡ºå¾ªç¯")
                                    break
                        
                        if done:
                            break
                    
                    # æ›´æ–°ç›®æ ‡ç½‘ç»œ
                    if episode % algorithm_params.get('target_update_frequency', 10) == 0:
                        agent.update_target_network()
                    
                    # æ›´æ–°æ¢ç´¢å‚æ•°
                    agent.update_exploration_params()
                    
                    episode_rewards.append(episode_reward)
                    episode_makespans.append(simulator.get_makespan())
                    
                    if episode % 10 == 0:
                        self.logger.info(f"    Episode {episode}: Reward={episode_reward:.2f}, Makespan={simulator.get_makespan():.2f}")
                        self.logger.info(f"      Total steps: {step_count}, Tasks scheduled: {len(simulator.get_schedule_history())}")
                
                # æœ€ç»ˆè¯„ä¼°
                final_metrics = {
                    'makespan': simulator.get_makespan(),
                    'resource_utilization': simulator.get_resource_utilization(),
                    'average_reward': np.mean(episode_rewards),
                    'final_episode_reward': episode_rewards[-1] if episode_rewards else 0,
                    'training_losses': agent.get_training_stats().get('losses', [])
                }
                
                results.append(final_metrics)
                
                # ä¸ºFE_IDDQNè®¾ç½®schedule_result
                schedule_result = {
                    'algorithm': algorithm_name,
                    'metrics': final_metrics,
                    'schedule_history': simulator.get_schedule_history()
                }

            elif algorithm_name == "DQN":
                state_size = 100  # ç¤ºä¾‹çŠ¶æ€ç»´åº¦
                action_size = simulator.num_resources
                agent = DQNScheduler(state_size, action_size, self.device)
                schedule_result = simulator.simulate_random_schedule(algorithm_name)

            elif algorithm_name == "DDQN":
                state_size = 100  # ç¤ºä¾‹çŠ¶æ€ç»´åº¦
                action_size = simulator.num_resources
                agent = DDQNScheduler(state_size, action_size, self.device)
                schedule_result = simulator.simulate_random_schedule(algorithm_name)
            elif algorithm_name == "BF_DDQN":
                state_size = 100  # ç¤ºä¾‹çŠ¶æ€ç»´åº¦
                action_size = simulator.num_resources
                agent = BF_DDQNScheduler(state_size, action_size, self.device)
                schedule_result = simulator.simulate_random_schedule(algorithm_name)
            elif algorithm_name == "FIFO":
                from baselines.traditional_schedulers import FIFOScheduler
                scheduler = FIFOScheduler()
                # è½¬æ¢ä¾èµ–å…³ç³»æ ¼å¼
                dependencies = [(dep['pre_task'], dep['post_task']) 
                              for dep in simulator.dependencies 
                              if dep['pre_task'] is not None and dep['post_task'] is not None]
                schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
                
                # å°†è°ƒåº¦ç»“æœè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                if 'error' not in schedule_result:
                    # è®¡ç®—makespanå’Œèµ„æºåˆ©ç”¨ç‡
                    if 'task_end_times' in schedule_result and schedule_result['task_end_times']:
                        makespan = max(schedule_result['task_end_times'].values())
                    else:
                        makespan = 0
                    
                    # è®¡ç®—èµ„æºåˆ©ç”¨ç‡
                    total_work = sum(task['duration'] for task in simulator.tasks)
                    total_capacity = makespan * len(simulator.resources) if makespan > 0 else 1
                    resource_utilization = total_work / total_capacity if total_capacity > 0 else 0
                    
                    schedule_result.update({
                        'makespan': makespan,
                        'resource_utilization': resource_utilization
                    })
                
            elif algorithm_name == "SJF":
                from baselines.traditional_schedulers import SJFScheduler
                scheduler = SJFScheduler()
                dependencies = [(dep['pre_task'], dep['post_task']) 
                              for dep in simulator.dependencies 
                              if dep['pre_task'] is not None and dep['post_task'] is not None]
                schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
                
                # å°†è°ƒåº¦ç»“æœè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                if 'error' not in schedule_result:
                    # è®¡ç®—makespanå’Œèµ„æºåˆ©ç”¨ç‡
                    if 'task_end_times' in schedule_result and schedule_result['task_end_times']:
                        makespan = max(schedule_result['task_end_times'].values())
                    else:
                        makespan = 0
                    
                    # è®¡ç®—èµ„æºåˆ©ç”¨ç‡
                    total_work = sum(task['duration'] for task in simulator.tasks)
                    total_capacity = makespan * len(simulator.resources) if makespan > 0 else 1
                    resource_utilization = total_work / total_capacity if total_capacity > 0 else 0
                    
                    schedule_result.update({
                        'makespan': makespan,
                        'resource_utilization': resource_utilization
                    })
                
            elif algorithm_name == "HEFT":
                from baselines.traditional_schedulers import HEFTScheduler
                scheduler = HEFTScheduler()
                dependencies = [(dep['pre_task'], dep['post_task']) 
                              for dep in simulator.dependencies 
                              if dep['pre_task'] is not None and dep['post_task'] is not None]
                schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
                
                # å°†è°ƒåº¦ç»“æœè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                if 'error' not in schedule_result:
                    # è®¡ç®—makespanå’Œèµ„æºåˆ©ç”¨ç‡
                    if 'task_end_times' in schedule_result and schedule_result['task_end_times']:
                        makespan = max(schedule_result['task_end_times'].values())
                    else:
                        makespan = 0
                    
                    # è®¡ç®—èµ„æºåˆ©ç”¨ç‡
                    total_work = sum(task['duration'] for task in simulator.tasks)
                    total_capacity = makespan * len(simulator.resources) if makespan > 0 else 1
                    resource_utilization = total_work / total_capacity if total_capacity > 0 else 0
                    
                    schedule_result.update({
                        'makespan': makespan,
                        'resource_utilization': resource_utilization
                    })
            elif algorithm_name == "GA":
                from baselines.meta_heuristics import GAScheduler
                scheduler = GAScheduler()
                dependencies = [(dep['pre_task'], dep['post_task']) 
                              for dep in simulator.dependencies 
                              if dep['pre_task'] is not None and dep['post_task'] is not None]
                schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
            elif algorithm_name == "PSO":
                from baselines.meta_heuristics import PSOScheduler
                scheduler = PSOScheduler()
                dependencies = [(dep['pre_task'], dep['post_task']) 
                              for dep in simulator.dependencies 
                              if dep['pre_task'] is not None and dep['post_task'] is not None]
                schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
            elif algorithm_name == "ACO":
                from baselines.meta_heuristics import ACOScheduler
                scheduler = ACOScheduler()
                dependencies = [(dep['pre_task'], dep['post_task']) 
                              for dep in simulator.dependencies 
                              if dep['pre_task'] is not None and dep['post_task'] is not None]
                schedule_result = scheduler.schedule(simulator.tasks, simulator.resources, dependencies)
            else:
                raise ValueError(f"Unknown algorithm: {algorithm_name}")

            # è¯„ä¼°ç»“æœ
            metrics = self.evaluator.evaluate(schedule_result)
            results.append(metrics)

        avg_results = self._calculate_average_metrics(results)
        self.logger.info(f"  Average metrics for {algorithm_name}: {avg_results}")
        return {algorithm_name: avg_results}

    def run_comparison_experiments(self, algorithms: List[str]) -> Dict[str, Dict[str, Any]]:
        """è¿è¡Œå¤šä¸ªç®—æ³•çš„å¯¹æ¯”å®éªŒ"""
        all_results = {}
        for algo in algorithms:
            all_results.update(self.run_algorithm(algo))
        return all_results

    def _calculate_average_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—å¤šæ¬¡å®éªŒçš„å¹³å‡æŒ‡æ ‡"""
        if not results: return {}

        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„é”®
        all_keys = set()
        for result in results:
            all_keys.update(result.keys())
        
        avg_metrics = {}
        for key in all_keys:
            # æ”¶é›†æ‰€æœ‰ç»“æœä¸­è¯¥é”®çš„å€¼
            values = []
            for result in results:
                if key in result:
                    values.append(result[key])
            
            if values:
                # å¦‚æœæ‰€æœ‰å€¼éƒ½æ˜¯æ•°å€¼ç±»å‹ï¼Œè®¡ç®—å¹³å‡å€¼
                if all(isinstance(v, (int, float)) for v in values):
                    avg_metrics[key] = np.mean(values)
                else:
                    # éæ•°å€¼å‹ç›´æ¥å–ç¬¬ä¸€ä¸ª
                    avg_metrics[key] = values[0]
            else:
                # å¦‚æœæŸä¸ªé”®åœ¨æ‰€æœ‰ç»“æœä¸­éƒ½ä¸å­˜åœ¨ï¼Œè®¾ä¸ºé»˜è®¤å€¼
                avg_metrics[key] = 0.0
        
        return avg_metrics

    def generate_comparison_report(self, all_results: Dict[str, Dict[str, Any]]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼ŒåŒ…æ‹¬å›¾è¡¨å’Œè¡¨æ ¼"""
        self.logger.info("Generating comparison report...")

        # å°†ç»“æœè½¬æ¢ä¸ºDataFrameæ–¹ä¾¿å¤„ç†
        results_df = pd.DataFrame.from_dict(all_results, orient='index')

        # ä¿å­˜åˆ°CSV
        table_path = Config.get_table_file_path("comparison_metrics")
        results_df.to_csv(table_path)
        self.logger.info(f"Comparison metrics saved to {table_path}")

        # å¯è§†åŒ– (éœ€è¦å®ç°utils.visualization)
        # from utils.visualization import plot_radar_chart, plot_bar_chart
        # plot_radar_chart(results_df, Config.get_figure_file_path("radar_chart"))
        # plot_bar_chart(results_df, Config.get_figure_file_path("bar_chart"))

        self.logger.info("Comparison report generated.")