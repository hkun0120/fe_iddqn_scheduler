#!/usr/bin/env python3
"""
æ”¹è¿›çš„FE-IDDQNç®—æ³•
åŸºäºå¥–åŠ±å‡½æ•°è®¾è®¡æ–‡æ¡£ï¼Œæ·»åŠ ä»¥ä¸‹æ”¹è¿›ï¼š
1. Reward Shaping (å¥–åŠ±å¡‘å½¢)
2. Prioritized Experience Replay (PER)
3. Generalized Advantage Estimation (GAE)
4. Curiosity-driven Exploration
5. æ”¹è¿›çš„ç½‘ç»œæ¶æ„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import Tuple, Optional, Dict, List
from collections import deque
import random

from models.dual_stream_network import DualStreamNetwork
from models.prioritized_replay_buffer import PrioritizedReplayBuffer
from config.hyperparameters import Hyperparameters


class PotentialFunction:
    """
    åŠ¿å‡½æ•° Î¦(s) ç”¨äºå¥–åŠ±å¡‘å½¢
    Î¦(s) = -é¢„æœŸå‰©ä½™æ—¶é—´
    
    ç›®çš„ï¼šå¼•å¯¼agentä¼˜å…ˆé€‰æ‹©èƒ½å¿«é€Ÿå®Œæˆworkflowçš„åŠ¨ä½œ
    """
    
    def __init__(self):
        self.gamma = 0.99
    
    def calculate(self, state: Tuple[np.ndarray, np.ndarray], 
                  completed_tasks: int, total_tasks: int) -> float:
        """
        è®¡ç®—çŠ¶æ€åŠ¿å‡½æ•°å€¼
        
        Args:
            state: (task_features, resource_features)
            completed_tasks: å·²å®Œæˆä»»åŠ¡æ•°
            total_tasks: æ€»ä»»åŠ¡æ•°
        
        Returns:
            åŠ¿å‡½æ•°å€¼ï¼ˆè´Ÿçš„é¢„æœŸå‰©ä½™æ—¶é—´ï¼‰
        """
        task_features, resource_features = state
        
        # å‰©ä½™ä»»åŠ¡æ¯”ä¾‹
        remaining_ratio = (total_tasks - completed_tasks) / total_tasks if total_tasks > 0 else 0
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´ï¼ˆåŸºäºä»»åŠ¡ç‰¹å¾ä¸­çš„durationï¼‰
        if len(task_features.shape) >= 2:
            # task_features shape: (num_tasks, 16)
            # ç¬¬3ç»´æ˜¯duration
            remaining_durations = task_features[:, 2] if task_features.shape[1] > 2 else np.zeros(len(task_features))
            estimated_remaining_time = np.sum(remaining_durations)
        else:
            # ç®€åŒ–ä¼°ç®—
            estimated_remaining_time = remaining_ratio * 1000.0  # å‡è®¾å¹³å‡ä»»åŠ¡20ç§’
        
        # åŠ¿å‡½æ•°ï¼šè´Ÿçš„å‰©ä½™æ—¶é—´ï¼ˆå‰©ä½™æ—¶é—´è¶Šå°‘ï¼ŒåŠ¿å‡½æ•°è¶Šå¤§ï¼‰
        phi = -estimated_remaining_time / 100.0  # å½’ä¸€åŒ–
        
        return phi


class ImprovedFE_IDDQN:
    """
    æ”¹è¿›çš„FE-IDDQNç®—æ³•
    
    ä¸»è¦æ”¹è¿›ï¼š
    1. Reward Shaping: R' = R + Î³*(Î¦(s') - Î¦(s))
    2. Prioritized Experience Replay
    3. Double DQN with Dueling Architecture
    4. Multi-step Learning
    5. Noisy Networks for Exploration
    """
    
    def __init__(self,
                 task_input_dim: int = 16,
                 resource_input_dim: int = 7,
                 action_dim: int = 6,
                 device: str = None):
        
        self.task_input_dim = task_input_dim
        self.resource_input_dim = resource_input_dim
        self.action_dim = action_dim
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è¶…å‚æ•°
        self.params = Hyperparameters.get_algorithm_params('FE_IDDQN')
        self.gamma = self.params.get('gamma', 0.99)
        self.learning_rate = self.params.get('learning_rate', 0.0001)
        self.batch_size = self.params.get('batch_size', 64)
        self.buffer_size = self.params.get('buffer_size', 100000)
        self.target_update_freq = self.params.get('target_update_freq', 100)
        
        # Multi-step learning
        self.n_step = 3  # 3-step returns
        self.n_step_buffer = deque(maxlen=self.n_step)
        
        # ç½‘ç»œ
        self.q_network = DualStreamNetwork(
            task_input_dim=task_input_dim,
            resource_input_dim=resource_input_dim,
            action_dim=action_dim,
            **{k: v for k, v in self.params.items() 
               if k in ['task_stream_hidden_dims', 'resource_stream_hidden_dims', 
                       'fusion_dim', 'attention_dim']}
        ).to(self.device)
        
        self.target_network = DualStreamNetwork(
            task_input_dim=task_input_dim,
            resource_input_dim=resource_input_dim,
            action_dim=action_dim,
            **{k: v for k, v in self.params.items() 
               if k in ['task_stream_hidden_dims', 'resource_stream_hidden_dims', 
                       'fusion_dim', 'attention_dim']}
        ).to(self.device)
        
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        # Prioritized Experience Replay
        self.replay_buffer = PrioritizedReplayBuffer(
            capacity=self.buffer_size,
            alpha=0.6,  # ä¼˜å…ˆçº§æŒ‡æ•°
            beta_start=0.4,  # é‡è¦æ€§é‡‡æ ·èµ·å§‹å€¼
            beta_frames=100000
        )
        
        # å¥–åŠ±å¡‘å½¢
        self.potential_function = PotentialFunction()
        self.use_reward_shaping = True
        
        # è®­ç»ƒç»Ÿè®¡
        self.train_step = 0
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        
    def select_action(self,
                     task_features: np.ndarray,
                     resource_features: np.ndarray,
                     exploration_strategy: str = 'epsilon_greedy',
                     epsilon: float = None) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Args:
            task_features: ä»»åŠ¡ç‰¹å¾
            resource_features: èµ„æºç‰¹å¾
            exploration_strategy: æ¢ç´¢ç­–ç•¥ ('epsilon_greedy', 'greedy', 'random')
            epsilon: Îµå€¼ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨self.epsilonï¼‰
        """
        if epsilon is None:
            epsilon = self.epsilon
        
        # Îµ-greedyæ¢ç´¢
        if exploration_strategy == 'epsilon_greedy' and random.random() < epsilon:
            return random.randint(0, self.action_dim - 1)
        
        if exploration_strategy == 'random':
            return random.randint(0, self.action_dim - 1)
        
        # Greedyé€‰æ‹©
        with torch.no_grad():
            task_tensor = torch.FloatTensor(task_features).unsqueeze(0).to(self.device)
            resource_tensor = torch.FloatTensor(resource_features).unsqueeze(0).to(self.device)
            
            q_values = self.q_network(task_tensor, resource_tensor)
            action = q_values.argmax(dim=1).item()
        
        return action
    
    def store_experience(self,
                        state: Tuple[np.ndarray, np.ndarray],
                        action: int,
                        reward: float,
                        next_state: Tuple[np.ndarray, np.ndarray],
                        done: bool,
                        shaped_reward: float = None):
        """
        å­˜å‚¨ç»éªŒï¼ˆæ”¯æŒå¥–åŠ±å¡‘å½¢ï¼‰
        
        Args:
            shaped_reward: å¦‚æœæä¾›ï¼Œä½¿ç”¨shaped rewardï¼›å¦åˆ™ä½¿ç”¨åŸå§‹reward
        """
        # ä½¿ç”¨shaped rewardï¼ˆå¦‚æœæä¾›ï¼‰
        final_reward = shaped_reward if shaped_reward is not None else reward
        
        # æ‹¼æ¥çŠ¶æ€
        task_features, resource_features = state
        state_array = np.concatenate([task_features.flatten(), resource_features.flatten()])
        
        next_task_features, next_resource_features = next_state
        next_state_array = np.concatenate([next_task_features.flatten(), next_resource_features.flatten()])
        
        # Multi-step learning
        self.n_step_buffer.append((state_array, action, final_reward, next_state_array, done))
        
        # å½“n-step bufferæ»¡æ—¶ï¼Œè®¡ç®—n-step returnå¹¶å­˜å‚¨
        if len(self.n_step_buffer) == self.n_step:
            # è®¡ç®—n-step return
            n_step_reward = 0.0
            gamma_power = 1.0
            
            for i, (_, _, r, _, _) in enumerate(self.n_step_buffer):
                n_step_reward += gamma_power * r
                gamma_power *= self.gamma
            
            # å–ç¬¬ä¸€ä¸ªçŠ¶æ€å’Œæœ€åä¸€ä¸ªnext_state
            first_state, first_action, _, _, _ = self.n_step_buffer[0]
            _, _, _, last_next_state, last_done = self.n_step_buffer[-1]
            
            # å­˜å…¥replay buffer
            self.replay_buffer.add(first_state, first_action, n_step_reward, 
                                  last_next_state, last_done)
    
    def apply_reward_shaping(self,
                            reward: float,
                            state: Tuple[np.ndarray, np.ndarray],
                            next_state: Tuple[np.ndarray, np.ndarray],
                            completed_tasks: int,
                            total_tasks: int) -> float:
        """
        åº”ç”¨å¥–åŠ±å¡‘å½¢
        
        R' = R + Î³*Î¦(s') - Î¦(s)
        
        å…¶ä¸­Î¦(s)æ˜¯åŠ¿å‡½æ•°ï¼ŒåŸºäºçŠ¶æ€çš„é¢„æœŸå‰©ä½™æ—¶é—´
        """
        if not self.use_reward_shaping:
            return reward
        
        # è®¡ç®—åŠ¿å‡½æ•°å€¼
        phi_current = self.potential_function.calculate(state, completed_tasks, total_tasks)
        phi_next = self.potential_function.calculate(next_state, completed_tasks + 1, total_tasks)
        
        # å¥–åŠ±å¡‘å½¢
        shaped_reward = reward + self.gamma * (phi_next - phi_current)
        
        return shaped_reward
    
    def train(self) -> Optional[float]:
        """
        è®­ç»ƒç½‘ç»œ - ä½¿ç”¨PERå’ŒDouble DQN
        """
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # ä»PERé‡‡æ ·
        states, actions, rewards, next_states, dones, is_weights, idxs = \
            self.replay_buffer.sample(self.batch_size)
        
        # è½¬æ¢ä¸ºtensor
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        is_weights = torch.FloatTensor(is_weights).to(self.device)
        
        # é‡æ„ç‰¹å¾
        task_features, resource_features = self._reconstruct_features(states)
        next_task_features, next_resource_features = self._reconstruct_features(next_states)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(task_features, resource_features)
        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # Double DQN: ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°
        with torch.no_grad():
            # ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
            next_q_main = self.q_network(next_task_features, next_resource_features)
            next_actions = next_q_main.argmax(dim=1)
            
            # ç›®æ ‡ç½‘ç»œè¯„ä¼°
            next_q_target = self.target_network(next_task_features, next_resource_features)
            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)
            
            # è®¡ç®—ç›®æ ‡Qå€¼
            target_q_values = rewards + (self.gamma ** self.n_step) * next_q_values * (~dones)
        
        # è®¡ç®—TDè¯¯å·®
        td_errors = torch.abs(current_q_values - target_q_values).detach().cpu().numpy()
        
        # æ›´æ–°PERä¼˜å…ˆçº§
        self.replay_buffer.update_priorities(idxs, td_errors + 1e-6)
        
        # è®¡ç®—åŠ æƒæŸå¤±
        loss = (is_weights * F.mse_loss(current_q_values, target_q_values, reduction='none')).mean()
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=10.0)
        
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.train_step += 1
        if self.train_step % self.target_update_freq == 0:
            self.update_target_network()
        
        # è¡°å‡epsilon
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        return loss.item()
    
    def _reconstruct_features(self, states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä»æ‹¼æ¥çš„çŠ¶æ€é‡æ„ä»»åŠ¡å’Œèµ„æºç‰¹å¾"""
        batch_size = states.shape[0]
        
        # å‡è®¾ä»»åŠ¡ç‰¹å¾ç»´åº¦
        max_tasks = 100  # æœ€å¤š100ä¸ªä»»åŠ¡
        task_feature_size = self.task_input_dim * max_tasks
        
        # åˆ†ç¦»ä»»åŠ¡å’Œèµ„æºç‰¹å¾
        task_features_flat = states[:, :task_feature_size]
        resource_features_flat = states[:, task_feature_size:]
        
        # é‡å¡‘ä¸ºæ­£ç¡®å½¢çŠ¶
        task_features = task_features_flat.reshape(batch_size, max_tasks, self.task_input_dim)
        resource_features = resource_features_flat.reshape(batch_size, -1, self.resource_input_dim)
        
        return task_features, resource_features
    
    def update_target_network(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        tau = 0.005  # è½¯æ›´æ–°ç³»æ•°
        for target_param, param in zip(self.target_network.parameters(), 
                                       self.q_network.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
    
    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_step': self.train_step,
            'epsilon': self.epsilon,
            'hyperparameters': self.params
        }, path)
    
    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_step = checkpoint.get('train_step', 0)
        self.epsilon = checkpoint.get('epsilon', 0.01)


class CuriosityModule(nn.Module):
    """
    å¥½å¥‡å¿ƒæ¨¡å— - ç”¨äºæ¢ç´¢æœªçŸ¥çŠ¶æ€
    
    åŸºäºICM (Intrinsic Curiosity Module)
    é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œé¢„æµ‹è¯¯å·®ä½œä¸ºå†…åœ¨å¥–åŠ±
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super().__init__()
        
        # Forward model: é¢„æµ‹s_{t+1}ç»™å®šs_tå’Œa_t
        self.forward_model = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, state_dim)
        )
        
        # Inverse model: é¢„æµ‹a_tç»™å®šs_tå’Œs_{t+1}
        self.inverse_model = nn.Sequential(
            nn.Linear(state_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        self.optimizer = optim.Adam(self.parameters(), lr=0.001)
    
    def forward(self, state, action, next_state):
        """
        è®¡ç®—å†…åœ¨å¥–åŠ±ï¼ˆé¢„æµ‹è¯¯å·®ï¼‰
        """
        # One-hot encode action
        action_onehot = F.one_hot(action, num_classes=self.forward_model[0].in_features - state.shape[1])
        
        # Forward modelé¢„æµ‹
        state_action = torch.cat([state, action_onehot.float()], dim=1)
        predicted_next_state = self.forward_model(state_action)
        
        # é¢„æµ‹è¯¯å·®ä½œä¸ºå†…åœ¨å¥–åŠ±
        intrinsic_reward = F.mse_loss(predicted_next_state, next_state, reduction='none').mean(dim=1)
        
        return intrinsic_reward
    
    def update(self, state, action, next_state):
        """æ›´æ–°å¥½å¥‡å¿ƒæ¨¡å—"""
        # Forward loss
        action_onehot = F.one_hot(action, num_classes=self.forward_model[0].in_features - state.shape[1])
        state_action = torch.cat([state, action_onehot.float()], dim=1)
        predicted_next_state = self.forward_model(state_action)
        forward_loss = F.mse_loss(predicted_next_state, next_state)
        
        # Inverse loss
        state_next_state = torch.cat([state, next_state], dim=1)
        predicted_action = self.inverse_model(state_next_state)
        inverse_loss = F.cross_entropy(predicted_action, action)
        
        # æ€»æŸå¤±
        total_loss = forward_loss + inverse_loss
        
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
        
        return total_loss.item()


class GAECalculator:
    """
    Generalized Advantage Estimation (GAE)
    ç”¨äºæ›´å‡†ç¡®çš„ä¼˜åŠ¿ä¼°è®¡
    """
    
    def __init__(self, gamma: float = 0.99, lambda_: float = 0.95):
        self.gamma = gamma
        self.lambda_ = lambda_
    
    def calculate_advantages(self, 
                            rewards: List[float], 
                            values: List[float], 
                            next_values: List[float],
                            dones: List[bool]) -> np.ndarray:
        """
        è®¡ç®—GAEä¼˜åŠ¿
        
        A_t = Î£_{l=0}^{âˆ} (Î³Î»)^l * Î´_{t+l}
        å…¶ä¸­ Î´_t = r_t + Î³*V(s_{t+1}) - V(s_t)
        """
        advantages = np.zeros(len(rewards))
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if dones[t]:
                delta = rewards[t] - values[t]
                advantages[t] = delta
                last_advantage = 0
            else:
                delta = rewards[t] + self.gamma * next_values[t] - values[t]
                advantages[t] = delta + self.gamma * self.lambda_ * last_advantage
                last_advantage = advantages[t]
        
        return advantages


def create_improved_trainer():
    """
    åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒå™¨
    
    åŒ…å«æ‰€æœ‰æ”¹è¿›ç»„ä»¶ï¼š
    - ImprovedFE_IDDQN (ä¸»ç®—æ³•)
    - PotentialFunction (å¥–åŠ±å¡‘å½¢)
    - CuriosityModule (æ¢ç´¢)
    - GAECalculator (ä¼˜åŠ¿ä¼°è®¡)
    """
    
    agent = ImprovedFE_IDDQN(
        task_input_dim=16,
        resource_input_dim=7,
        action_dim=6
    )
    
    # å¥½å¥‡å¿ƒæ¨¡å—ï¼ˆå¯é€‰ï¼‰
    state_dim = 16 * 100 + 7 * 10  # å‡è®¾æœ€å¤š100ä¸ªä»»åŠ¡ï¼Œ10ä¸ªèµ„æº
    curiosity = CuriosityModule(
        state_dim=state_dim,
        action_dim=6,
        hidden_dim=128
    )
    
    # GAEè®¡ç®—å™¨
    gae_calculator = GAECalculator(gamma=0.99, lambda_=0.95)
    
    return {
        'agent': agent,
        'curiosity': curiosity,
        'gae': gae_calculator
    }


if __name__ == "__main__":
    print("="*100)
    print("æ”¹è¿›çš„FE-IDDQNæ¨¡å‹ç»„ä»¶")
    print("="*100)
    
    components = create_improved_trainer()
    
    print(f"\nâœ… ImprovedFE_IDDQN: {components['agent']}")
    print(f"   - Q-Networkå‚æ•°: {sum(p.numel() for p in components['agent'].q_network.parameters()):,}")
    print(f"   - ä½¿ç”¨PER: âœ“")
    print(f"   - ä½¿ç”¨Reward Shaping: âœ“")
    print(f"   - Multi-step Learning: 3-step")
    
    print(f"\nâœ… CuriosityModule: {components['curiosity']}")
    print(f"   - å‚æ•°æ•°é‡: {sum(p.numel() for p in components['curiosity'].parameters()):,}")
    
    print(f"\nâœ… GAE Calculator: {components['gae']}")
    print(f"   - Î³={components['gae'].gamma}, Î»={components['gae'].lambda_}")
    
    print(f"\nğŸ“š æ”¹è¿›ç‚¹æ€»ç»“:")
    print(f"   1. âœ… Reward Shaping (åŠ¿å‡½æ•°å¼•å¯¼)")
    print(f"   2. âœ… Prioritized Experience Replay")
    print(f"   3. âœ… Double DQN")
    print(f"   4. âœ… Multi-step Learning (3-step)")
    print(f"   5. âœ… Curiosity-driven Exploration")
    print(f"   6. âœ… Gradient Clipping")
    print(f"   7. âœ… Soft Target Update")

